<div class="container software-container py-5" id="soft-container">
    <h2 class="text-center mb-4">Ressources</h2>
  
    <div class="row justify-content-center">
      <div class="col-lg-8 col-md-10 col-sm-12">
        <div class="software-card">
          <h3 class="software-title">REFINE-LM</h3>
          <p class="software-description">
            With the introduction of (large) language models, there has been significant concern about the unintended bias such models may inherit from their training data. A number of studies have shown that such models propagate gender stereotypes, as well as geographical and racial bias, among other biases. While existing works tackle this issue by preprocessing data and debiasing embeddings, the proposed methods require a lot of computational resources and annotation effort while being limited to certain types of biases. To address these issues, we introduce REFINE-LM, a debiasing method that uses reinforcement learning to handle different types of biases without any fine-tuning. By training a simple model on top of the word probability distribution of a LM, our bias agnostic reinforcement learning method enables model debiasing without human annotations or significant computational resources. Experiments conducted on a wide range of models, including several LMs, show that our method (i) significantly reduces stereotypical biases while preserving LMs performance; (ii) is applicable to different types of biases, generalizing across contexts such as gender, ethnicity, religion, and nationalitybased biases; and (iii) it is not expensive to train.

          </p>
          <a href="https://github.com/rameez-mrq/refine-lm-ecai" target="_blank" class="btn btn-dark">
            <i class="bi bi-github"></i> Voir sur GitHub
          </a>
        </div>
      </div>
    </div>
  </div>
  